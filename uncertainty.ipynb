{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Estimation\n",
    "  - **0:** class\n",
    "  - **1:** x_mean\n",
    "  - **2:** x_std\n",
    "  - **3:** y_mean\n",
    "  - **4:** y_std\n",
    "  - **5:** w_mean\n",
    "  - **6:** w_std\n",
    "  - **7:** h_mean\n",
    "  - **8:** h_std\n",
    "  - **9:** conf\n",
    "  - **10:** uncertainty\n",
    "  - **11** IoU\n",
    "\n",
    "  **pred** hold all the predicitons of all images. \\\n",
    "  **p** holds all predictions of a single image. \\\n",
    "  **p2** is a single prediction in an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics.utils.ops import xywh2xyxy\n",
    "from torchvision import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(predictions_path, gt_path):\n",
    "\n",
    "    predictions = []\n",
    "    gt = []\n",
    "\n",
    "    for file in os.listdir(predictions_path):\n",
    "        prediction_file_path = os.path.join(predictions_path, file)\n",
    "        prediction_name = os.path.splitext(file)[0]\n",
    "\n",
    "        # load ensemble predictions\n",
    "        try:\n",
    "            with open(prediction_file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                predictions.append(data)\n",
    "        # load ensemble member predictions\n",
    "        except:\n",
    "            with open(prediction_file_path, \"r\") as f:\n",
    "                p = []\n",
    "                for line in f:\n",
    "                    pred_line_data = [float(value) for value in line.split()]\n",
    "                    p.append([int(pred_line_data[0]), \n",
    "                                        pred_line_data[1], 0,\n",
    "                                        pred_line_data[2], 0,\n",
    "                                        pred_line_data[3], 0,\n",
    "                                        pred_line_data[4], 0,\n",
    "                                        pred_line_data[5]])\n",
    "            predictions.append(p)\n",
    "\n",
    "        # load ground truths\n",
    "        if gt_path:\n",
    "            with open(os.path.join(gt_path, prediction_name + \".txt\"), \"r\") as f:\n",
    "                g = []\n",
    "                for line in f:\n",
    "                    gt_line_data = [float(value) for value in line.split()]\n",
    "                    g.append(gt_line_data)\n",
    "            gt.append(g)\n",
    "\n",
    "    return predictions, gt\n",
    "\n",
    "def fuzzy(pred, std):\n",
    "    \n",
    "    for i in pred:\n",
    "\n",
    "        image = []\n",
    "\n",
    "        for p in i:\n",
    "\n",
    "            # x, y are coordinates\n",
    "            # w, h are distance\n",
    "            x, y, w, h = p[1], p[3], p[5], p[7]\n",
    "            x_std, y_std, w_std, h_std = p[2], p[4], p[6], p[8]\n",
    "\n",
    "            # add x standard diviations to the original w and h\n",
    "            w_fuzzy = w + w_std * std\n",
    "            h_fuzzy = h + h_std * std\n",
    "\n",
    "            # add the distance betwen points x,y and x,y+std to w and h\n",
    "            w_fuzzy += abs(x + x_std * std -  x)\n",
    "            h_fuzzy += abs(y + y_std * std -  y)\n",
    "\n",
    "            box1 = xywh2xyxy(torch.tensor([[x, y, w, h]], dtype=torch.float))\n",
    "            box2 = xywh2xyxy(torch.tensor([[x, y, w_fuzzy, h_fuzzy]], dtype=torch.float))\n",
    "            iou = 1 - ops.box_iou(box1, box2).numpy()[0][0]\n",
    "            p.append(iou)\n",
    "            image.append(p)\n",
    "\n",
    "def match(pred, gt):\n",
    "\n",
    "    for i, pred in enumerate(pred):\n",
    "        for p in pred:\n",
    "            for g in gt[i]:\n",
    "                box1 = xywh2xyxy(torch.tensor([[p[1], p[3] ,p[5] ,p[7]]], dtype=torch.float))\n",
    "                box2 = xywh2xyxy(torch.tensor([g[1:]], dtype=torch.float))\n",
    "                iou = ops.box_iou(box1, box2).numpy()[0][0]\n",
    "                \n",
    "                if p[0] == g[0] and iou > 0.55:\n",
    "                    p.append(iou)\n",
    "                    break\n",
    "\n",
    "def binning(pred):\n",
    "    bins = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    bins_conf = [[] for _ in range(len(bins) - 1)]\n",
    "\n",
    "        # bin confidence scores\n",
    "    for p in pred:\n",
    "        for p2 in p:\n",
    "            for j, bin_start in enumerate(bins):\n",
    "                bin_end = bins[j + 1]\n",
    "                if bin_start <= p2[9] < bin_end:\n",
    "                    if len(p2) == 11:\n",
    "                        bins_conf[j].append((p2[9], None))\n",
    "                    else:\n",
    "                        bins_conf[j].append((p2[9], p2[11]))\n",
    "                    break\n",
    "    \n",
    "    return bins_conf\n",
    "\n",
    "def calibration(conf_binned):\n",
    "    ece = 0\n",
    "    conf_means = []\n",
    "    positives_ratios = []\n",
    "    total_length = sum(len(bin) for bin in conf_binned)\n",
    "\n",
    "    for bin in conf_binned:\n",
    "\n",
    "        if not bin:\n",
    "            continue\n",
    "\n",
    "        # mean conf per bin\n",
    "        confidence = [conf[0] for conf in bin]\n",
    "        conf_mean = np.mean(confidence)\n",
    "        conf_means.append(conf_mean)\n",
    "\n",
    "        # TP per bin\n",
    "        positives = 0\n",
    "        for p in bin:\n",
    "            if p[1]:\n",
    "                positives += 1\n",
    "        \n",
    "        positives_ratio = positives / (len(bin) + 1e-16)\n",
    "        positives_ratios.append(positives_ratio)\n",
    "        ece += 1/total_length * len(bin) * abs(positives_ratio - conf_mean)\n",
    "\n",
    "    return conf_means, positives_ratios, ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Save Metrics\n",
    " - Loads the predictions and ground truths into two lists.\n",
    " - Adds a fuzzy uncertainty value at the end of each prediction.\n",
    " - Add a IoU score at the end for each correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "\n",
    "# pred, gt = load(\"YOLOv9c_predictions_0.15\\\\1\\labels\", \"datasets\\crystals\\labels\\\\test\")\n",
    "# fuzzy(pred, 1)\n",
    "# match(pred, gt)\n",
    "\n",
    "\n",
    "# pred, gt = load(\"YOLOv9c_predictions_0.01\\ensemble_10\", \"datasets\\crystals\\labels\\\\test\")\n",
    "# fuzzy(pred, 1)\n",
    "# match(pred, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALIBRATION PLOT: model/ensemble error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'plots_uncertainty\\calibration_accuracy_conf_model_ensemble'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(1, m + 1):\n",
    "    pred, gt = load(f\"YOLOv9c_predictions_0.01\\{i}\\labels\", \"datasets\\crystals\\labels\\\\test\")\n",
    "    fuzzy(pred, 1)\n",
    "    match(pred, gt)\n",
    "\n",
    "    conf_binned = binning(pred)\n",
    "    conf_means, positives_ratios, ece = calibration(conf_binned)\n",
    "\n",
    "    plt.plot(conf_means, positives_ratios, linewidth=1, marker='o', markersize=2, label=f'Model {i} (ECE: {ece:.3f})')\n",
    "\n",
    "\n",
    "pred, gt = load(f\"YOLOv9c_predictions_0.01\\ensemble_10\", \"datasets\\crystals\\labels\\\\test\")\n",
    "fuzzy(pred, 1)\n",
    "match(pred, gt)\n",
    "conf_binned = binning(pred)\n",
    "conf_means, positives_ratios, ece = calibration(conf_binned)\n",
    "\n",
    "plt.plot(conf_means, positives_ratios, linewidth=2, marker='o', markersize=4, label=f'Ensemble (ECE: {ece:.3f})', color='black')\n",
    "plt.plot([0, 1], [0, 1], color='0.7', linestyle='--')\n",
    "plt.xlabel('Means of Binned Confidences')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title(f\"m = {i}\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(os.path.join(directory, f'{i}.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALIBRATION PLOT: ensemble error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'plots_uncertainty\\calibration_accuracy_conf_ensemble'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(1, m + 1):\n",
    "    pred, gt = load(f\"YOLOv9c_predictions_0.01\\ensemble_{i}\", \"datasets\\crystals\\labels\\\\test\")\n",
    "    fuzzy(pred, 1)\n",
    "    match(pred, gt)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    conf_binned = binning(pred)\n",
    "    conf_means, positives_ratios, ece = calibration(conf_binned)\n",
    "\n",
    "    plt.plot(conf_means, positives_ratios, linewidth=2, marker='o', markersize=5,label=f'ECE: {ece:.3f}', color='black')\n",
    "    plt.plot([0, 1], [0, 1], color='0.7', linestyle='--')\n",
    "    plt.xlabel('Means of Binned Confidences')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f\"m = {i}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(os.path.join(directory, f'{i}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCATTER PLOT: error_conf\n",
    "Error as a function of confidence, highlighting correct classifications (green), misclassifications (red), and the number of ensemble members (m).\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'plots_uncertainty\\scatter_error_conf'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(1, m + 1):\n",
    "    pred, gt = load(f\"YOLOv9c_predictions_0.01\\ensemble_{i}\", \"datasets\\crystals\\labels\\\\test\")\n",
    "    fuzzy(pred, 1)\n",
    "    match(pred, gt)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    data = [(subsub[9], subsub[10], len(subsub)) for sublist in pred for subsub in sublist if len(subsub) in {11, 12}]\n",
    "    x, y, subsub_length = zip(*data)\n",
    "    colors = ['green' if length == 12 else 'red' for length in subsub_length]\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    trendline = np.poly1d(coefficients)\n",
    "\n",
    "    plt.figure(dpi=500)\n",
    "    plt.scatter(x, y, c=colors, s=5)\n",
    "    plt.plot(x, trendline(x), color='black', linestyle='-', label='Trendline')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f\"m = {i}\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.45)\n",
    "    plt.savefig(os.path.join(directory, f'{i}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCATTER PLOT: error_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'plots_uncertainty\\scatter_error_iou'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(1, m + 1):\n",
    "    pred, gt = load(f\"YOLOv9c_predictions_0.01\\ensemble_{i}\", \"datasets\\crystals\\labels\\\\test\")\n",
    "    fuzzy(pred, 1)\n",
    "    match(pred, gt)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    data = [(subsub[11], subsub[10], len(subsub)) for sublist in pred for subsub in sublist if len(subsub) in {12}]\n",
    "    x, y, subsub_length = zip(*data)\n",
    "    colors = ['green' if length == 12 else 'red' for length in subsub_length]\n",
    "    coefficients = np.polyfit(x, y, 1)\n",
    "    trendline = np.poly1d(coefficients)\n",
    "\n",
    "    plt.figure(dpi=500)\n",
    "    plt.scatter(x, y, c=colors, s=5)\n",
    "    plt.plot(x, trendline(x), color='black', linestyle='-', label='Trendline')\n",
    "    plt.xlabel('IoU')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f\"m = {i}\")\n",
    "    plt.xlim(0.55, 1)\n",
    "    plt.ylim(0, 0.45)\n",
    "    plt.savefig(os.path.join(directory, f'{i}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD: marco vs coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directory = 'plots_uncertainty\\histogram_ood_0.01_noother'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "for i in range(1, m + 1):\n",
    "\n",
    "    pred_id, _ = load(f\"YOLOv9c_predictions_0.01\\ensemble_{i}\", None)\n",
    "    pred_ood, _ = load(f\"YOLOv9c_predictions_0.01_coco\\ensemble_{i}\", None)\n",
    "    fuzzy(pred_id, 2)\n",
    "    match(pred_id, gt)\n",
    "    fuzzy(pred_ood, 2)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    error_id = []\n",
    "    error_ood = []\n",
    "    for id, ood in zip(pred_id, pred_ood):\n",
    "        for j in id:\n",
    "            if j[0] != 0 and j[0] != 5:\n",
    "                error_id.append(j[10])\n",
    "        for k in ood:\n",
    "            if k[0] != 0 and k[0] != 5:\n",
    "                error_ood.append(k[10]) \n",
    "\n",
    "    plt.figure(dpi=500)\n",
    "    bin_edges = np.linspace(0, 1, 51)[1:]\n",
    "    plt.hist(error_id, bins=bin_edges, density=True, alpha=0.5, label='In-Distribution', edgecolor='black', color='green')\n",
    "    plt.hist(error_ood, bins=bin_edges, density=True, alpha=0.5, label='Out-of-Distribution', edgecolor='black', color='red')\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"m = {i}\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 12)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(directory, f'{i}.png'))\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
